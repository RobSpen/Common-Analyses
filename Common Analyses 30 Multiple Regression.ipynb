{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f2859e",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "\n",
    "\n",
    "Multiple regression analysis is an extension of linear regression where, instead of just having one independent variable (IV), we have two or more independent variables. When fitting a multiple regression model we are assessing the relationship between a linear combination of the IVs on a scale dependent variable (DV). With simple linear regression we were able to plot the relationship between our IV (x-axis variable) and our DV (y-axis variable) using the cartesian coordinate plane. once we have more than two IVs in a multiple regression model it becomes very difficult to visualise as we would have a plot with greater than three dimensions, however, we can still mathematically fit the model and work out the respective coefficients (slopes) for the different IVs. We can also use the output from a multiple regression model to assess the relative importance of the different IVs as predictors of scores on the DV. \n",
    "\n",
    "Recall that the equation for linear regression was derived from the algebraic equation for a straight line. Where a score on the DV is a function of the intercept (the offset point where the regression line crosses the y-axis when 0 is inputted for the x (IV) variable) plus the slope of the regression line multipled by the IV score: \n",
    "\n",
    "- DV score = intercept + slope * IV score\n",
    "- y = ${\\alpha}$ + ${\\beta}$ * x\n",
    "\n",
    "With multiple regression this equation is extended by simply adding more IVs to the right hand side of the equation. Each additional IV will have its own slope relative to the DV. Below is an example with three IVs:\n",
    "\n",
    "- DV score = intercept + slope1 * IV1 score + slope2 * IV2 score + slope3 * IV3 score\n",
    "- y = ${\\alpha}$ + ${\\beta}$1 * x1 + ${\\beta}$2 * x2 + ${\\beta}$3 * x3\n",
    "\n",
    "In this notebook I will run a multiple regression analysis to predict socres on a DV using two scale IVs. I will use the statsmodels software library to achieve this. Statsmodels allows us to fit regression models using formulas from the R programming language that closely match the format of the regression equation above and are intuitively easy to understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf80d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing key software libraries.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af85c4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>books</th>\n",
       "      <th>attend</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   books  attend  grade\n",
       "0      0       9     45\n",
       "1      1      15     57\n",
       "2      0      10     45\n",
       "3      2      16     51\n",
       "4      4      10     65"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing a dataset to use for the analysis. \n",
    "\n",
    "df = pd.read_csv(\"R1.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e8fe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40 entries, 0 to 39\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   books   40 non-null     int64\n",
      " 1   attend  40 non-null     int64\n",
      " 2   grade   40 non-null     int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 1.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ea8d7",
   "metadata": {},
   "source": [
    "We can see that this is a small dataset consisting of observations from 40 participants on a university course recording how many books they read, how many classes they attended, and the grade for the course. All three variables are integer type variables as they are measured on a discrete scale, as such they are suitable for a multiple regression analysis. \n",
    "\n",
    "I will fit a multiple regression model to this data to see if student's grades (DV score) can be predicted by their attendance of classes (IV1) and the number of textbooks they have read (IV2). The model will also allow us to assess which of these IVs is the best predictor of grade achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad4c2271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>grade</td>      <th>  R-squared:         </th> <td>   0.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 08 Sep 2023</td> <th>  Prob (F-statistic):</th> <td>0.000628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:16:17</td>     <th>  Log-Likelihood:    </th> <td> -160.91</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    40</td>      <th>  AIC:               </th> <td>   327.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    37</td>      <th>  BIC:               </th> <td>   332.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   37.3792</td> <td>    7.745</td> <td>    4.827</td> <td> 0.000</td> <td>   21.687</td> <td>   53.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>attend</th>    <td>    1.2835</td> <td>    0.587</td> <td>    2.187</td> <td> 0.035</td> <td>    0.094</td> <td>    2.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>books</th>     <td>    4.0369</td> <td>    1.753</td> <td>    2.303</td> <td> 0.027</td> <td>    0.485</td> <td>    7.589</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.082</td> <th>  Durbin-Watson:     </th> <td>   1.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.353</td> <th>  Jarque-Bera (JB):  </th> <td>   1.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.221</td> <th>  Prob(JB):          </th> <td>   0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.187</td> <th>  Cond. No.          </th> <td>    52.0</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  grade   R-squared:                       0.329\n",
       "Model:                            OLS   Adj. R-squared:                  0.292\n",
       "Method:                 Least Squares   F-statistic:                     9.059\n",
       "Date:                Fri, 08 Sep 2023   Prob (F-statistic):           0.000628\n",
       "Time:                        14:16:17   Log-Likelihood:                -160.91\n",
       "No. Observations:                  40   AIC:                             327.8\n",
       "Df Residuals:                      37   BIC:                             332.9\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     37.3792      7.745      4.827      0.000      21.687      53.071\n",
       "attend         1.2835      0.587      2.187      0.035       0.094       2.473\n",
       "books          4.0369      1.753      2.303      0.027       0.485       7.589\n",
       "==============================================================================\n",
       "Omnibus:                        2.082   Durbin-Watson:                   1.764\n",
       "Prob(Omnibus):                  0.353   Jarque-Bera (JB):                1.427\n",
       "Skew:                           0.221   Prob(JB):                        0.490\n",
       "Kurtosis:                       2.187   Cond. No.                         52.0\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a multiple regression model using statsmodels. \n",
    "# Note that the formula specification uses the tilda (~) character to separate the DV and IVs\n",
    "\n",
    "mod_1 = smf.ols(formula = \"grade ~ attend + books\", data = df).fit()\n",
    "\n",
    "mod_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f65ffac",
   "metadata": {},
   "source": [
    "The above output shows that the regression model is significant. The F-value associated with the model is significant indicating that fitting this model is a better way to predict scores on the DV than just using the mean DV value. \n",
    "\n",
    "F(2,37) = 9.06, p < 0.001\n",
    "\n",
    "In the model, participant’s predicted test grade was equal to: grade =  37.38 + 4.04*(books) + 1.28*(attendance).\n",
    "Using the intercept and coefficients for our model we can input different values for the IVs (books read and classes attended) to get a predicted grade for a student that has read any given number of books and attended any number of classes. \n",
    "\n",
    "We can summarise the result of this model as: Participants test grade increased 4.04 points for each book read and 1.28 points for each class attended.  Positive changes in test grade were associated with more books read and more frequent attendance.\n",
    "\n",
    "The R-squared value for the model captures the model fit. This gives us some measure of how much variability in scores on the DV is accounted for by changes in scores on the IV. For this model R-squared was 0.33, indicating about 33% of variability in scores on the DV could be accounted for by changes in scores on the IVs. Using canned rules-of-thumb for effect sizes we would consider this a medium sized effect. This suggest that we have a reasonably good model for predicting grades using these two variables, although there is still a lot of variability unaccounted for that will be explained by other vairables that we have not measured or included in the model. \n",
    "\n",
    "We can also see from the associated t-tests for each IV whether they were significant predictors in the model. In this case, both IVs were significant and contributed significantly to the prediction of scores on the DV. Attendance: t(37) = 2.19, p = 0.04; Books: t(37) = 2.30, p = 0.03. \n",
    "\n",
    "One issue with the above model is that, although both IVs were significant, we cannot assess their relative importance as predictors of the DV. We cannot use the unstandardised ${\\beta}$ coefficients to estimate this as the size of each coefficient will be influenced by the scale it was measured on. So, an IV could have a larger unstandarised ${\\beta}$ coefficient but be making a smaller contribution to the model as a predictor of the DV. \n",
    "\n",
    "There are a couple of ways we can assess the relative importance of the different IVs in the model. One way is to run a similar multiple regression analysis using the pingouin software library and set a parameter of the model, called relimp, equal to True. Doing this will partition the R-squared value for the model into the relative contribution made up of each IV. This will also give the percentage of the R-squared value contributed by each IV. \n",
    "\n",
    "A second approach to assess the relative importance of different IVs is to standarise the variables in the dataframe prior to running the multiple regression analysis in statsmodels. I will demonstrate both of these methods in the below cells.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6dbd6d",
   "metadata": {},
   "source": [
    "### Assessing the relative importance of predictor IVs using pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "073326f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>coef</th>\n",
       "      <th>se</th>\n",
       "      <th>T</th>\n",
       "      <th>pval</th>\n",
       "      <th>r2</th>\n",
       "      <th>adj_r2</th>\n",
       "      <th>CI[2.5%]</th>\n",
       "      <th>CI[97.5%]</th>\n",
       "      <th>relimp</th>\n",
       "      <th>relimp_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>37.379185</td>\n",
       "      <td>7.744564</td>\n",
       "      <td>4.826506</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.328712</td>\n",
       "      <td>0.292426</td>\n",
       "      <td>21.687207</td>\n",
       "      <td>53.071163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attend</td>\n",
       "      <td>1.283477</td>\n",
       "      <td>0.586964</td>\n",
       "      <td>2.186636</td>\n",
       "      <td>0.035168</td>\n",
       "      <td>0.328712</td>\n",
       "      <td>0.292426</td>\n",
       "      <td>0.094175</td>\n",
       "      <td>2.472780</td>\n",
       "      <td>0.159626</td>\n",
       "      <td>48.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>books</td>\n",
       "      <td>4.036893</td>\n",
       "      <td>1.753049</td>\n",
       "      <td>2.302784</td>\n",
       "      <td>0.027015</td>\n",
       "      <td>0.328712</td>\n",
       "      <td>0.292426</td>\n",
       "      <td>0.484878</td>\n",
       "      <td>7.588907</td>\n",
       "      <td>0.169086</td>\n",
       "      <td>51.439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       names       coef        se         T      pval        r2    adj_r2  \\\n",
       "0  Intercept  37.379185  7.744564  4.826506  0.000024  0.328712  0.292426   \n",
       "1     attend   1.283477  0.586964  2.186636  0.035168  0.328712  0.292426   \n",
       "2      books   4.036893  1.753049  2.302784  0.027015  0.328712  0.292426   \n",
       "\n",
       "    CI[2.5%]  CI[97.5%]    relimp  relimp_perc  \n",
       "0  21.687207  53.071163       NaN          NaN  \n",
       "1   0.094175   2.472780  0.159626       48.561  \n",
       "2   0.484878   7.588907  0.169086       51.439  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running regression analysis in Pingouin to obtain relative importance (relimp) of individual IVs. \n",
    "\n",
    "mod_2 = pg.linear_regression(df[['attend', 'books']], df['grade'], relimp = True)\n",
    "\n",
    "mod_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fae08c",
   "metadata": {},
   "source": [
    "Above is the coefficients table for the model produced by the pingouin linear_regression method. This returns the same values for the intercept and coefficients for the IVs as in the previous model fitted using statsmodels. Some additional information has been requested in this table, this is the relative importance (relimp) of each IV. As stated above, this is the proportion of the R-squared variance explained in the DV that is attributable to each IV. We can see from the relimp column that, of the approximately 33% of variance explained by the model, about 16% (0.159) is explained by attendance and about 17% (0.169) is explained by books read. In this case both IVs seem to be contributing fairly equally to the model as predictors. The relimp_perc column gives these relative contributions stated as a percentage of the total R-squared value. Here we can see that books accounts for slightly more variance explained (51.5%) compared to attendance (48.5%). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c34183",
   "metadata": {},
   "source": [
    "### Standardising variables to obtain a standardized regression model\n",
    "\n",
    "An alternative way to assess the contribution of individual predictors to the multiple regression model is to first standardise the variables used in the model and then run the analysis on the standardised data. This will ensure that all variables are recorded on the same scale and can be directly compared by looking at their coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e3f7c",
   "metadata": {},
   "source": [
    "Standardised regression coefficients provide an easy way to estimate effect size and contribution to the model that is independent of the units the variable was originally measured in. Here I will use scipy stats to convert the variables to z-scores and then create a pandas series object. I will create two new objects, one for the DV and one for the IVs, and then run the analysis again using statsmodels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b408eb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -1.124557\n",
       "1   -0.397081\n",
       "2   -1.124557\n",
       "3   -0.760819\n",
       "4    0.087903\n",
       "Name: grade, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing scipy and using the z-score method. I have wrapped this inside a pandas Series so that the object created is\n",
    "# a dataframe containing only one variable. \n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "grade_norm = pd.Series(stats.zscore(df['grade']))\n",
    "grade_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c826fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attend</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean  std dev\n",
       "const    1.0      0.0\n",
       "attend   0.0      1.0\n",
       "books    0.0      1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I am converting the IVs to standardised z-scores. \n",
    "# First, creating a new object containing only the IVs (books and attend).\n",
    "# Converting that object (ivs_norm) to a dataframe containing the standardised IVs.\n",
    "# Next adding a constant to the dataframe. This will be all values of 1 to act as the coefficient for the intercept.\n",
    "# Then adding a check to see the mean and standard deviation for the standardised IVs. \n",
    "# If standardisation has work each IV should have a mean of 0 and standard deviation of 1. \n",
    "# The constant will have a mean of 1 and standard deviation of 0 as all values of that variable are the same. \n",
    "\n",
    "ivs_norm = df[['attend', 'books']]\n",
    "ivs_norm = pd.DataFrame(stats.zscore(ivs_norm))\n",
    "ivs_norm = sm.add_constant(ivs_norm)\n",
    "\n",
    "check = pd.concat([round(ivs_norm.mean(axis=0), 5), round(ivs_norm.std(axis=0, ddof=0), 5)], axis=1)\n",
    "check.columns=[\"mean\", \"std dev\"]\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62295e",
   "metadata": {},
   "source": [
    "The standardisation appears to have worked and the IVs now have a mean of 0 and sd of 1. Next I will fit the mutliple regression model again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bead4cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>grade</td>      <th>  R-squared:         </th> <td>   0.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 08 Sep 2023</td> <th>  Prob (F-statistic):</th> <td>0.000628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:35:15</td>     <th>  Log-Likelihood:    </th> <td> -48.786</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    40</td>      <th>  AIC:               </th> <td>   103.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    37</td>      <th>  BIC:               </th> <td>   108.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td> 1.388e-16</td> <td>    0.135</td> <td> 1.03e-15</td> <td> 1.000</td> <td>   -0.273</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>attend</th> <td>    0.3286</td> <td>    0.150</td> <td>    2.187</td> <td> 0.035</td> <td>    0.024</td> <td>    0.633</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>books</th>  <td>    0.3461</td> <td>    0.150</td> <td>    2.303</td> <td> 0.027</td> <td>    0.042</td> <td>    0.651</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.082</td> <th>  Durbin-Watson:     </th> <td>   1.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.353</td> <th>  Jarque-Bera (JB):  </th> <td>   1.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.221</td> <th>  Prob(JB):          </th> <td>   0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.187</td> <th>  Cond. No.          </th> <td>    1.61</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  grade   R-squared:                       0.329\n",
       "Model:                            OLS   Adj. R-squared:                  0.292\n",
       "Method:                 Least Squares   F-statistic:                     9.059\n",
       "Date:                Fri, 08 Sep 2023   Prob (F-statistic):           0.000628\n",
       "Time:                        14:35:15   Log-Likelihood:                -48.786\n",
       "No. Observations:                  40   AIC:                             103.6\n",
       "Df Residuals:                      37   BIC:                             108.6\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1.388e-16      0.135   1.03e-15      1.000      -0.273       0.273\n",
       "attend         0.3286      0.150      2.187      0.035       0.024       0.633\n",
       "books          0.3461      0.150      2.303      0.027       0.042       0.651\n",
       "==============================================================================\n",
       "Omnibus:                        2.082   Durbin-Watson:                   1.764\n",
       "Prob(Omnibus):                  0.353   Jarque-Bera (JB):                1.427\n",
       "Skew:                           0.221   Prob(JB):                        0.490\n",
       "Kurtosis:                       2.187   Cond. No.                         1.61\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardised multiple regression model. \n",
    "\n",
    "mod_std = sm.OLS(grade_norm, ivs_norm).fit()\n",
    "\n",
    "mod_std.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939885f",
   "metadata": {},
   "source": [
    "In the above output we see that the model returns the same result in terms of signficance (F(2,37) = 9.06, p < 0.001) and fit (R-squared = 0.33). However, on this occasion we see that the coefficients have changed and are in standardised units. Attendance has a coefficient of 0.33 and books has a coefficient of 0.35. This tells us that every one standard deviation increase in attendance results in an increase in grade of about 0.33 standard deviations, and every one standard deviation increase in books produces about a 0.34 standard deviation increase in grade. Both IVs are still significant predictors (Attendance: t(37) = 2.19, p = 0.04; Books: t(37) = 2.30, p = 0.03) and, as we observed when checking the relative importance of these two variables, are both contributing fairly equally to the variance explained in DVs scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cab66",
   "metadata": {},
   "source": [
    "A good way to visualise the relative contribution of our different IVs to the model is by using a tornado diagram to plot the standardised regression coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b933b3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMZElEQVR4nO3cfYylZX2H8etbRmV5KRR3qRRcpoopiqCFqRFEaisxaP/YNm20dluapXGLxprGuLGBZGvaEtvSpClJm3a1KC81jZJgbQ1QS1KoLFRmYV+ggqGICpjwIipkLVL99Y+5J4zTmZ3nnDMz5zl6fZIJ5zznuc/5zc2wF885u5uqQpKkHxv3AJKkfjAIkiTAIEiSGoMgSQIMgiSpmRr3AKPYuHFjTU9Pj3sMSZooe/bseaKqNi0+PtFBmJ6eZnZ2dtxjSNJESfKVpY77lpEkCTAIkqTGIEiSAIMgSWoMgiQJMAiSpGaif9vpFx9+krN2XD3uMSRpXe25/MI1eV6vECRJgEGQJDUGQZIEGARJUmMQJEmAQZAkNQZBkgQYBElSYxAkSYBBkCQ1BkGSBBgESVJjECRJgEGQJDUGQZIEGARJUmMQJEmAQZAkNQZBkgQYBElSYxAkSYBBkCQ1BkGSBBgESVIzcBCSTCe5Z9QXTvJQko2jPo8kaXV4hSBJAoYPwlSSq5LsT3JdkiOSvDnJ3UkOJLkyyYsAljs+L8mGJDcmeVeSI5N8Nsm+JPckecfI36EkqZNhg/AzwK6qOgP4NvB+4OPAO6rqdGAKeHeSw5c6vuB5jgL+GfhEVX0EuAB4tKpeU1WvBm4ccj5J0oCGDcLXquq2dvta4M3Al6vqS+3YVcB5zIVjqePz/gn4WFVd3e4fAM5P8mdJ3lhV31r8wkm2J5lNMvu/B58ecnxJ0mLDBqE6npcVHr8NeGuSALRwnMVcGD6cZOf/e+GqXVU1U1UzU0ccPcjMkqRDGDYIm5Oc3W6/E/g3YDrJKe3YbwG3APctc3zeTuBJ4G8AkvwUcLCqrgX+AjhzyPkkSQMaNghfBH47yX7gOOAvgW3Ap5IcAL4P/G1V/c9Sxxc91+8Dhyf5c+B04AtJ9gKXAn8y5HySpAFNDbqgqh4CXrXEQzcDP7vE+csdn15wd9uC2zcNOpMkaXT+OQRJEmAQJEmNQZAkAQZBktQYBEkSYBAkSY1BkCQBBkGS1BgESRJgECRJjUGQJAEGQZLUGARJEmAQJEmNQZAkAQZBktQYBEkSYBAkSY1BkCQBBkGS1BgESRJgECRJjUGQJAEGQZLUTI17gFG88qQXM3v5heMeQ5J+KHiFIEkCDIIkqTEIkiTAIEiSGoMgSQIMgiSpMQiSJMAgSJIagyBJAgyCJKkxCJIkwCBIkhqDIEkCJvxvO/3u1+/lq390+rjHkKSBbN55YNwjLMkrBEkSYBAkSY1BkCQBBkGS1BgESRJgECRJjUGQJAEGQZLUGARJEmAQJEmNQZAkAQZBktQYBEkSYBAkSY1BkCQBBkGS1BgESRJgECRJjUGQJAEGQZLUGARJEmAQJEmNQZAkAQZBktQYBEkSMEIQklyy4PaxSd6zOiNBkukk96zW80mSVjbKFcIlC24fC6xaECRJ62+qy0lJPg28FDgc+CvgZcCGJHuBe4HDgJe3+5+rqh1JdgBvB14EXF9Vf5hkGrgB+DxwDvAIsKWqvpPkLOBK4GB7XJK0jjoFAbioqr6RZANwJ/DzwHur6rUw9xYP8OoF998CvAJ4HRDgM0nOA77ajr+zqt6V5JPArwLXAh8Dfq+qbkly+XKDJNkObAc48ZgXDPbdSpKW1fUto/cl2QfcwdyVwitWOP8t7etu4C7g1AVrvlxVe9vtPcB0kmOAY6vqlnb8muWeuKp2VdVMVc0cd+RhHceXJK1kxSuEJG8CzgfOrqqDSf6dubeODrkM+HBV/d2i55oGnl1w6HvAhnZ+dR1akrT6ulwhHAM81WJwKvD6dvy5JPPv2TwNHL1gzU3ARUmOAkhyYpLjl3uBqvom8K0k57ZDWwf4HiRJq6DLZwg3Ahcn2Q/cz9zbRgC7gP1J7qqqrUlua79V9Ib2ofIrgduTADwD/CZzVwTL2QZcmeQgc0GRJK2jVE3uOzVnnLih/uV3Txn3GJI0kM07D4z19ZPsqaqZxcf9k8qSJMAgSJIagyBJAgyCJKkxCJIkwCBIkhqDIEkCDIIkqTEIkiTAIEiSGoMgSQIMgiSpMQiSJMAgSJIagyBJAgyCJKkxCJIkwCBIkhqDIEkCDIIkqTEIkiTAIEiSGoMgSQJgatwDjOKFJ5zG5p2z4x5Dkn4oeIUgSQIMgiSpMQiSJMAgSJIagyBJAgyCJKkxCJIkwCBIkhqDIEkCDIIkqTEIkiTAIEiSGoMgSQIMgiSpMQiSJMAgSJIagyBJAgyCJKkxCJIkwCBIkhqDIEkCDIIkqTEIkiTAIEiSGoMgSQIMgiSpMQiSJMAgSJIagyBJAgyCJKkxCJIkwCBIkhqDIEkCDIIkqTEIkiTAIEiSGoMgSQIMgiSpMQiSJKCHQUhyybhnkKQfRb0LAmAQJGkMhg5CkguT7E+yL8k1SU5OcnM7dnOSze28jye5IsnuJA8m+bV2/IQktybZm+SeJG9M8qfAhnbsH1bpe5QkdTA1zKIkpwGXAm+oqieSHAdcBVxdVVcluQi4AvjltuQE4FzgVOAzwHXAbwA3VdVlSQ4Djqiq/0jy3qp67SjflCRpcMNeIfwicF1VPQFQVd8AzgY+0R6/hrkAzPt0VX2/qv4L+Ml27E5gW5IPAadX1dNdXjjJ9iSzSWYff/zxIceXJC02bBAC1ArnLHz82UVrqapbgfOAR4BrklzY5YWraldVzVTVzKZNmwYYWZJ0KMMG4Wbg7UleDNDeMtoN/Hp7fCvw+UM9QZKTgceq6iPA3wNntoeeS/KCIeeSJA1pqM8QqureJJcBtyT5HnA38D7gyiQ7gMeBbSs8zZuAHUmeA54B5q8QdgH7k9xVVVuHmU+SNLhUrfTOT3/NzMzU7OzsuMeQpImSZE9VzSw+3sc/hyBJGgODIEkCDIIkqTEIkiTAIEiSGoMgSQIMgiSpMQiSJMAgSJIagyBJAgyCJKkxCJIkwCBIkhqDIEkCDIIkqTEIkiTAIEiSGoMgSQIMgiSpMQiSJMAgSJIagyBJAgyCJKkxCJIkwCBIkhqDIEkCDIIkqTEIkiTAIEiSGoMgSQIMgiSpSVWNe4ahJXkauH/ccwxhI/DEuIcYgnOvL+deXz9Kc59cVZsWH5xanXnG5v6qmhn3EINKMuvc68e515dzr6/VnNu3jCRJgEGQJDWTHoRd4x5gSM69vpx7fTn3+lq1uSf6Q2VJ0uqZ9CsESdIqMQiSJKCnQUhyQZL7kzyQ5A+WeDxJrmiP709yZte1PZ77oSQHkuxNMruec7fXX2n2U5PcnuTZJB8YZO1aGnHuse15h7m3tp+R/Ul2J3lN17U9nrvP+72lzbw3yWySc7uu7fHcg+93VfXqCzgM+G/gZcALgX3Aqxad8zbgBiDA64H/7Lq2j3O3xx4CNvZ4z48Hfg64DPjAIGv7OPc497zj3OcAP9Fuv3WCfsaXnHsC9vsonv9M9QzgvgnZ7yXnHna/+3iF8Drggap6sKq+C/wjsGXROVuAq2vOHcCxSU7ouLaPc4/birNX1WNVdSfw3KBr19Aoc49Tl7l3V9VT7e4dwEld1/Z07nHqMvcz1X4VBY4Equvans49lD4G4UTgawvuP9yOdTmny9q1MsrcMPcv8l+T7Emyfc2mXNoo+9b3PT+Uce35oHP/DnNXlsOsXU2jzA093+8kv5LkPuCzwEWDrF0jo8wNQ+x3H//qiixxbHH1ljuny9q1MsrcAG+oqkeTHA98Lsl9VXXrqk64vFH2re97fijj2vPOcyf5BeZ+YZ1/b3gi9nuJuaHn+11V1wPXJzkP+GPg/K5r18goc8MQ+93HK4SHgZcuuH8S8GjHc7qsXSujzE1Vzf/zMeB65i4X18so+9b3PV/WGPe809xJzgA+CmypqicHWbtGRpm79/s9r/2i+fIkGwddu8pGmXu4/V6PD0cG/CBlCngQ+Gme/yDltEXn/BI/+OHsF7qu7encRwJHL7i9G7igT3u+4NwP8YMfKvd6zw8x99j2vOPPymbgAeCcYb/nns3d9/0+hec/nD0TeKT9d9r3/V5u7qH2e82/qSE34m3Al5j7hP3Sduxi4OJ2O8Bft8cPADOHWtv3uZn7XQT72te96z13x9lfwtz/sXwb+Ga7/eMTsOdLzj3uPe8w90eBp4C97Wt2Qn7Gl5x7Avb7g22uvcDtwLkTst9Lzj3sfvtXV0iSgH5+hiBJGgODIEkCDIIkqTEIkiTAIEiSGoMgSQIMgiSp+T9sQJ2RfV/AuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "coeff = mod_std.params\n",
    "coeff = coeff.iloc[(coeff.abs()*-1.0).argsort()]\n",
    "sns.barplot(x=coeff.values, y=coeff.index, orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ccd7b",
   "metadata": {},
   "source": [
    "The above plot confirms what we already though to be the case when inspecting the standardised coefficients in the model output, that both books and attendance contribute fairly evenly to predicting grade scores. Tornado plots like this are very useful when we have lot sof IVs in our model, in which case we can see their relative weighting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243cfed3",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "- Multiple regression extends the linear regression model by allowing us to add more IVs.\n",
    "- Multiple regression analysis can be conducted using statsmodels and this allows us to use R style formula specifications.\n",
    "- Some or all of our IVs may be contributing to a regression model that is significant and can be used to predict scores on the DV.\n",
    "- We can assess whether individual IVs are significant predictors by checking their associated t-test.\n",
    "- To assess the relative contribution of different IVs as predictors in the model we can use pingouin to assess their relative importance by partitioning their contribution to the variance explained (R-squared).\n",
    "- Alternatively, we can standardise our variables of interest so they are all measured on the same scale and then fit the regression model using the standardised versions of the variables. This allows us to directly compare different IVs and their relationship to the DV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
